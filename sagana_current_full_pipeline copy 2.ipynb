{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Full Pipeline For BMI detection using extra features extracted as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# conda create -n bmi-predictor python=3.10 -y\n",
    "# conda activate bmi-predictor\n",
    "\n",
    "# # For MPS on macOS with ARM (M1/M2/M3)\n",
    "# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cpu\n",
    "\n",
    "# pip install facenet-pytorch\n",
    "# pip install pandas scikit-learn matplotlib tqdm jupyter notebook\n",
    "# pip install opencv-python pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core count: 10\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# STEP 0: Imports and Setup\n",
    "# ===============================================\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from facenet_pytorch import InceptionResnetV1, MTCNN\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm import tqdm \n",
    "\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Core count: {os.cpu_count()}')\n",
    "torch.set_num_threads(os.cpu_count())  # Use all available CPU cores\n",
    "DEVICE = torch.device(\"cpu\")  # Force CPU usage for portability\n",
    "print(\"Using device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4206 rows from CSV\n",
      "Filtered to 3962 rows with existing images\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bmi</th>\n",
       "      <th>gender</th>\n",
       "      <th>is_training</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.207396</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>img_0.bmp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26.453720</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>img_1.bmp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34.967561</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "      <td>img_2.bmp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22.044766</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "      <td>img_3.bmp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>25.845588</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "      <td>img_6.bmp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         bmi  gender  is_training       name\n",
       "0  34.207396    Male            1  img_0.bmp\n",
       "1  26.453720    Male            1  img_1.bmp\n",
       "2  34.967561  Female            1  img_2.bmp\n",
       "3  22.044766  Female            1  img_3.bmp\n",
       "6  25.845588  Female            1  img_6.bmp"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paths\n",
    "IMAGE_DIR = \"data/Images\"\n",
    "existing_image_files = os.listdir(IMAGE_DIR)\n",
    "\n",
    "# Load CSV\n",
    "data_df = pd.read_csv(\"data/data.csv\").drop(columns=[\"Unnamed: 0\"])\n",
    "print(f\"Loaded {data_df.shape[0]} rows from CSV\")\n",
    "\n",
    "# Filter out rows with missing image files\n",
    "data_df = data_df[data_df[\"name\"].isin(existing_image_files)]\n",
    "print(f\"Filtered to {data_df.shape[0]} rows with existing images\")\n",
    "\n",
    "del existing_image_files\n",
    "data_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jaw_width</th>\n",
       "      <th>face_height</th>\n",
       "      <th>cheekbone_width</th>\n",
       "      <th>nose_width</th>\n",
       "      <th>mouth_width</th>\n",
       "      <th>mouth_height</th>\n",
       "      <th>eye_distance</th>\n",
       "      <th>left_eye_width</th>\n",
       "      <th>right_eye_width</th>\n",
       "      <th>face_width_to_height</th>\n",
       "      <th>mouth_to_nose_ratio</th>\n",
       "      <th>image</th>\n",
       "      <th>race</th>\n",
       "      <th>pred_gender</th>\n",
       "      <th>age</th>\n",
       "      <th>key</th>\n",
       "      <th>bmi</th>\n",
       "      <th>gender</th>\n",
       "      <th>is_training</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>237.002110</td>\n",
       "      <td>210.287898</td>\n",
       "      <td>237.008439</td>\n",
       "      <td>52.009614</td>\n",
       "      <td>120.016666</td>\n",
       "      <td>58.034473</td>\n",
       "      <td>62.008064</td>\n",
       "      <td>46.173586</td>\n",
       "      <td>42.107007</td>\n",
       "      <td>1.127036</td>\n",
       "      <td>2.307586</td>\n",
       "      <td>img_1066_face0.jpg</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>20-29</td>\n",
       "      <td>img_1066</td>\n",
       "      <td>41.191406</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "      <td>img_1066.bmp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>226.035395</td>\n",
       "      <td>200.010000</td>\n",
       "      <td>220.002273</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>104.076895</td>\n",
       "      <td>38.052595</td>\n",
       "      <td>62.072538</td>\n",
       "      <td>43.011626</td>\n",
       "      <td>41.048752</td>\n",
       "      <td>1.130120</td>\n",
       "      <td>2.040723</td>\n",
       "      <td>img_1585_face0.jpg</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>20-29</td>\n",
       "      <td>img_1585</td>\n",
       "      <td>24.658895</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>img_1585.bmp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>241.101638</td>\n",
       "      <td>209.193690</td>\n",
       "      <td>239.075302</td>\n",
       "      <td>42.011903</td>\n",
       "      <td>87.005747</td>\n",
       "      <td>32.062439</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>42.107007</td>\n",
       "      <td>43.011626</td>\n",
       "      <td>1.152528</td>\n",
       "      <td>2.070978</td>\n",
       "      <td>img_3852_face0.jpg</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>20-29</td>\n",
       "      <td>img_3852</td>\n",
       "      <td>39.151259</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>img_3852.bmp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>229.490741</td>\n",
       "      <td>198.494332</td>\n",
       "      <td>229.176788</td>\n",
       "      <td>53.037722</td>\n",
       "      <td>80.056230</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>54.083269</td>\n",
       "      <td>42.011903</td>\n",
       "      <td>41.012193</td>\n",
       "      <td>1.156158</td>\n",
       "      <td>1.509421</td>\n",
       "      <td>img_1857_face0.jpg</td>\n",
       "      <td>East Asian</td>\n",
       "      <td>Male</td>\n",
       "      <td>30-39</td>\n",
       "      <td>img_1857</td>\n",
       "      <td>25.845588</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>img_1857.bmp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>235.552542</td>\n",
       "      <td>201.613492</td>\n",
       "      <td>235.766834</td>\n",
       "      <td>48.041649</td>\n",
       "      <td>77.524190</td>\n",
       "      <td>27.892651</td>\n",
       "      <td>60.299254</td>\n",
       "      <td>34.014703</td>\n",
       "      <td>41.194660</td>\n",
       "      <td>1.168337</td>\n",
       "      <td>1.613687</td>\n",
       "      <td>img_4196_face0.jpg</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>20-29</td>\n",
       "      <td>img_4196</td>\n",
       "      <td>36.243556</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>img_4196.bmp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    jaw_width  face_height  cheekbone_width  nose_width  mouth_width  \\\n",
       "0  237.002110   210.287898       237.008439   52.009614   120.016666   \n",
       "1  226.035395   200.010000       220.002273   51.000000   104.076895   \n",
       "2  241.101638   209.193690       239.075302   42.011903    87.005747   \n",
       "3  229.490741   198.494332       229.176788   53.037722    80.056230   \n",
       "4  235.552542   201.613492       235.766834   48.041649    77.524190   \n",
       "\n",
       "   mouth_height  eye_distance  left_eye_width  right_eye_width  \\\n",
       "0     58.034473     62.008064       46.173586        42.107007   \n",
       "1     38.052595     62.072538       43.011626        41.048752   \n",
       "2     32.062439     65.000000       42.107007        43.011626   \n",
       "3     12.000000     54.083269       42.011903        41.012193   \n",
       "4     27.892651     60.299254       34.014703        41.194660   \n",
       "\n",
       "   face_width_to_height  mouth_to_nose_ratio               image        race  \\\n",
       "0              1.127036             2.307586  img_1066_face0.jpg       White   \n",
       "1              1.130120             2.040723  img_1585_face0.jpg       White   \n",
       "2              1.152528             2.070978  img_3852_face0.jpg       White   \n",
       "3              1.156158             1.509421  img_1857_face0.jpg  East Asian   \n",
       "4              1.168337             1.613687  img_4196_face0.jpg       White   \n",
       "\n",
       "  pred_gender    age       key        bmi  gender  is_training          name  \n",
       "0      Female  20-29  img_1066  41.191406  Female            1  img_1066.bmp  \n",
       "1        Male  20-29  img_1585  24.658895    Male            1  img_1585.bmp  \n",
       "2      Female  20-29  img_3852  39.151259  Female            0  img_3852.bmp  \n",
       "3        Male  30-39  img_1857  25.845588    Male            1  img_1857.bmp  \n",
       "4        Male  20-29  img_4196  36.243556    Male            0  img_4196.bmp  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df = pd.read_csv('data/Feature_Add.csv')\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset shape: (3712, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bmi</th>\n",
       "      <th>is_training</th>\n",
       "      <th>name</th>\n",
       "      <th>jaw_width</th>\n",
       "      <th>face_height</th>\n",
       "      <th>cheekbone_width</th>\n",
       "      <th>nose_width</th>\n",
       "      <th>mouth_width</th>\n",
       "      <th>mouth_height</th>\n",
       "      <th>eye_distance</th>\n",
       "      <th>left_eye_width</th>\n",
       "      <th>right_eye_width</th>\n",
       "      <th>face_width_to_height</th>\n",
       "      <th>mouth_to_nose_ratio</th>\n",
       "      <th>race</th>\n",
       "      <th>pred_gender</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.207396</td>\n",
       "      <td>1</td>\n",
       "      <td>img_0.bmp</td>\n",
       "      <td>242.008264</td>\n",
       "      <td>176.045449</td>\n",
       "      <td>239.002092</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>97.020616</td>\n",
       "      <td>18.027756</td>\n",
       "      <td>62.008064</td>\n",
       "      <td>44.045431</td>\n",
       "      <td>43.046487</td>\n",
       "      <td>1.374692</td>\n",
       "      <td>1.940412</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>30-39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26.453720</td>\n",
       "      <td>1</td>\n",
       "      <td>img_1.bmp</td>\n",
       "      <td>227.140925</td>\n",
       "      <td>185.067555</td>\n",
       "      <td>220.056811</td>\n",
       "      <td>48.041649</td>\n",
       "      <td>88.051122</td>\n",
       "      <td>22.022716</td>\n",
       "      <td>59.008474</td>\n",
       "      <td>41.012193</td>\n",
       "      <td>43.185646</td>\n",
       "      <td>1.227341</td>\n",
       "      <td>1.832808</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>20-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34.967561</td>\n",
       "      <td>1</td>\n",
       "      <td>img_2.bmp</td>\n",
       "      <td>269.185809</td>\n",
       "      <td>217.057596</td>\n",
       "      <td>266.030073</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>103.019416</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>59.033889</td>\n",
       "      <td>42.011903</td>\n",
       "      <td>44.045431</td>\n",
       "      <td>1.240158</td>\n",
       "      <td>2.146238</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>20-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22.044766</td>\n",
       "      <td>1</td>\n",
       "      <td>img_3.bmp</td>\n",
       "      <td>244.018442</td>\n",
       "      <td>216.148097</td>\n",
       "      <td>239.002092</td>\n",
       "      <td>44.045431</td>\n",
       "      <td>102.044108</td>\n",
       "      <td>50.039984</td>\n",
       "      <td>59.008474</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>41.012193</td>\n",
       "      <td>1.128941</td>\n",
       "      <td>2.316792</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>20-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25.845588</td>\n",
       "      <td>1</td>\n",
       "      <td>img_6.bmp</td>\n",
       "      <td>238.838858</td>\n",
       "      <td>196.063765</td>\n",
       "      <td>236.541751</td>\n",
       "      <td>48.041649</td>\n",
       "      <td>93.085982</td>\n",
       "      <td>24.020824</td>\n",
       "      <td>69.260378</td>\n",
       "      <td>48.259714</td>\n",
       "      <td>47.010637</td>\n",
       "      <td>1.218169</td>\n",
       "      <td>1.937610</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>10-19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         bmi  is_training       name   jaw_width  face_height  \\\n",
       "0  34.207396            1  img_0.bmp  242.008264   176.045449   \n",
       "1  26.453720            1  img_1.bmp  227.140925   185.067555   \n",
       "2  34.967561            1  img_2.bmp  269.185809   217.057596   \n",
       "3  22.044766            1  img_3.bmp  244.018442   216.148097   \n",
       "4  25.845588            1  img_6.bmp  238.838858   196.063765   \n",
       "\n",
       "   cheekbone_width  nose_width  mouth_width  mouth_height  eye_distance  \\\n",
       "0       239.002092   50.000000    97.020616     18.027756     62.008064   \n",
       "1       220.056811   48.041649    88.051122     22.022716     59.008474   \n",
       "2       266.030073   48.000000   103.019416     51.000000     59.033889   \n",
       "3       239.002092   44.045431   102.044108     50.039984     59.008474   \n",
       "4       236.541751   48.041649    93.085982     24.020824     69.260378   \n",
       "\n",
       "   left_eye_width  right_eye_width  face_width_to_height  mouth_to_nose_ratio  \\\n",
       "0       44.045431        43.046487              1.374692             1.940412   \n",
       "1       41.012193        43.185646              1.227341             1.832808   \n",
       "2       42.011903        44.045431              1.240158             2.146238   \n",
       "3       41.000000        41.012193              1.128941             2.316792   \n",
       "4       48.259714        47.010637              1.218169             1.937610   \n",
       "\n",
       "    race pred_gender    age  \n",
       "0  White        Male  30-39  \n",
       "1  White        Male  20-29  \n",
       "2  White      Female  20-29  \n",
       "3  White      Female  20-29  \n",
       "4  White      Female  10-19  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.merge(data_df, features_df, on=['gender', 'bmi', 'name', 'is_training'], how='inner')\n",
    "df.drop(columns=['key', 'image', 'gender'], inplace=True)\n",
    "print(\"Combined dataset shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bmi</th>\n",
       "      <th>is_training</th>\n",
       "      <th>name</th>\n",
       "      <th>jaw_width</th>\n",
       "      <th>face_height</th>\n",
       "      <th>cheekbone_width</th>\n",
       "      <th>nose_width</th>\n",
       "      <th>mouth_width</th>\n",
       "      <th>mouth_height</th>\n",
       "      <th>eye_distance</th>\n",
       "      <th>left_eye_width</th>\n",
       "      <th>right_eye_width</th>\n",
       "      <th>face_width_to_height</th>\n",
       "      <th>mouth_to_nose_ratio</th>\n",
       "      <th>race</th>\n",
       "      <th>pred_gender</th>\n",
       "      <th>age</th>\n",
       "      <th>bmi_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.207396</td>\n",
       "      <td>1</td>\n",
       "      <td>img_0.bmp</td>\n",
       "      <td>242.008264</td>\n",
       "      <td>176.045449</td>\n",
       "      <td>239.002092</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>97.020616</td>\n",
       "      <td>18.027756</td>\n",
       "      <td>62.008064</td>\n",
       "      <td>44.045431</td>\n",
       "      <td>43.046487</td>\n",
       "      <td>1.374692</td>\n",
       "      <td>1.940412</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>30-39</td>\n",
       "      <td>0.191720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26.453720</td>\n",
       "      <td>1</td>\n",
       "      <td>img_1.bmp</td>\n",
       "      <td>227.140925</td>\n",
       "      <td>185.067555</td>\n",
       "      <td>220.056811</td>\n",
       "      <td>48.041649</td>\n",
       "      <td>88.051122</td>\n",
       "      <td>22.022716</td>\n",
       "      <td>59.008474</td>\n",
       "      <td>41.012193</td>\n",
       "      <td>43.185646</td>\n",
       "      <td>1.227341</td>\n",
       "      <td>1.832808</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>20-29</td>\n",
       "      <td>-0.762228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34.967561</td>\n",
       "      <td>1</td>\n",
       "      <td>img_2.bmp</td>\n",
       "      <td>269.185809</td>\n",
       "      <td>217.057596</td>\n",
       "      <td>266.030073</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>103.019416</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>59.033889</td>\n",
       "      <td>42.011903</td>\n",
       "      <td>44.045431</td>\n",
       "      <td>1.240158</td>\n",
       "      <td>2.146238</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>20-29</td>\n",
       "      <td>0.285244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22.044766</td>\n",
       "      <td>1</td>\n",
       "      <td>img_3.bmp</td>\n",
       "      <td>244.018442</td>\n",
       "      <td>216.148097</td>\n",
       "      <td>239.002092</td>\n",
       "      <td>44.045431</td>\n",
       "      <td>102.044108</td>\n",
       "      <td>50.039984</td>\n",
       "      <td>59.008474</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>41.012193</td>\n",
       "      <td>1.128941</td>\n",
       "      <td>2.316792</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>20-29</td>\n",
       "      <td>-1.304670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25.845588</td>\n",
       "      <td>1</td>\n",
       "      <td>img_6.bmp</td>\n",
       "      <td>238.838858</td>\n",
       "      <td>196.063765</td>\n",
       "      <td>236.541751</td>\n",
       "      <td>48.041649</td>\n",
       "      <td>93.085982</td>\n",
       "      <td>24.020824</td>\n",
       "      <td>69.260378</td>\n",
       "      <td>48.259714</td>\n",
       "      <td>47.010637</td>\n",
       "      <td>1.218169</td>\n",
       "      <td>1.937610</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>10-19</td>\n",
       "      <td>-0.837048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         bmi  is_training       name   jaw_width  face_height  \\\n",
       "0  34.207396            1  img_0.bmp  242.008264   176.045449   \n",
       "1  26.453720            1  img_1.bmp  227.140925   185.067555   \n",
       "2  34.967561            1  img_2.bmp  269.185809   217.057596   \n",
       "3  22.044766            1  img_3.bmp  244.018442   216.148097   \n",
       "4  25.845588            1  img_6.bmp  238.838858   196.063765   \n",
       "\n",
       "   cheekbone_width  nose_width  mouth_width  mouth_height  eye_distance  \\\n",
       "0       239.002092   50.000000    97.020616     18.027756     62.008064   \n",
       "1       220.056811   48.041649    88.051122     22.022716     59.008474   \n",
       "2       266.030073   48.000000   103.019416     51.000000     59.033889   \n",
       "3       239.002092   44.045431   102.044108     50.039984     59.008474   \n",
       "4       236.541751   48.041649    93.085982     24.020824     69.260378   \n",
       "\n",
       "   left_eye_width  right_eye_width  face_width_to_height  mouth_to_nose_ratio  \\\n",
       "0       44.045431        43.046487              1.374692             1.940412   \n",
       "1       41.012193        43.185646              1.227341             1.832808   \n",
       "2       42.011903        44.045431              1.240158             2.146238   \n",
       "3       41.000000        41.012193              1.128941             2.316792   \n",
       "4       48.259714        47.010637              1.218169             1.937610   \n",
       "\n",
       "    race pred_gender    age  bmi_norm  \n",
       "0  White        Male  30-39  0.191720  \n",
       "1  White        Male  20-29 -0.762228  \n",
       "2  White      Female  20-29  0.285244  \n",
       "3  White      Female  20-29 -1.304670  \n",
       "4  White      Female  10-19 -0.837048  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize target variable\n",
    "bmi_mean = df['bmi'].mean()\n",
    "bmi_std = df['bmi'].std()\n",
    "df['bmi_norm'] = (df['bmi'] - bmi_mean) / bmi_std\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entire data shape: (3712, 18)\n",
      "Gender Distribution Over Entire Dataset:\n",
      "  pred_gender  percentage\n",
      "0        Male    0.581088\n",
      "1      Female    0.418912\n",
      "\n",
      "Training data shape: (3008, 18)\n",
      "Gender Distribution Over Training Dataset:\n",
      "  pred_gender  percentage\n",
      "0        Male    0.590093\n",
      "1      Female    0.409907\n",
      "\n",
      "Testing data shape: (704, 18)\n",
      "Gender Distribution Over Testing Dataset:\n",
      "  pred_gender  percentage\n",
      "0        Male    0.542614\n",
      "1      Female    0.457386\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### General information ####\n",
    "print(f\"Entire data shape: {df.shape}\")\n",
    "gender_total = df['pred_gender'].value_counts() / len(df)\n",
    "gender_total = gender_total.reset_index()\n",
    "gender_total.rename(columns={'count': 'percentage'}, inplace=True)\n",
    "print(f\"Gender Distribution Over Entire Dataset:\\n{gender_total}\\n\")\n",
    "\n",
    "# Split into training and testing sets\n",
    "training_data_df = df[df['is_training'] == 1]\n",
    "testing_data_df = df[df['is_training'] == 0]\n",
    "\n",
    "gender_training_data_df = training_data_df['pred_gender'].value_counts() / len(training_data_df)\n",
    "gender_training_data_df = gender_training_data_df.reset_index()\n",
    "gender_training_data_df.rename(columns={'count': 'percentage'}, inplace=True)\n",
    "print(f\"Training data shape: {training_data_df.shape}\")\n",
    "print(f\"Gender Distribution Over Training Dataset:\\n{gender_training_data_df}\\n\")\n",
    "\n",
    "gender_testing_data_df = testing_data_df['pred_gender'].value_counts() / len(testing_data_df)\n",
    "gender_testing_data_df = gender_testing_data_df.reset_index()\n",
    "gender_testing_data_df.rename(columns={'count': 'percentage'}, inplace=True)\n",
    "print(f\"Testing data shape: {testing_data_df.shape}\")\n",
    "print(f\"Gender Distribution Over Testing Dataset:\\n{gender_testing_data_df}\\n\")\n",
    "\n",
    "del data_df, features_df, gender_total, training_data_df, testing_data_df, gender_training_data_df, gender_testing_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bmi</th>\n",
       "      <th>is_training</th>\n",
       "      <th>name</th>\n",
       "      <th>jaw_width</th>\n",
       "      <th>face_height</th>\n",
       "      <th>cheekbone_width</th>\n",
       "      <th>nose_width</th>\n",
       "      <th>mouth_width</th>\n",
       "      <th>mouth_height</th>\n",
       "      <th>eye_distance</th>\n",
       "      <th>...</th>\n",
       "      <th>race_Latino_Hispanic</th>\n",
       "      <th>race_White</th>\n",
       "      <th>gender_Female</th>\n",
       "      <th>gender_Male</th>\n",
       "      <th>age_10-19</th>\n",
       "      <th>age_20-29</th>\n",
       "      <th>age_3-9</th>\n",
       "      <th>age_30-39</th>\n",
       "      <th>age_40-49</th>\n",
       "      <th>age_50-59</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.207396</td>\n",
       "      <td>1</td>\n",
       "      <td>img_0.bmp</td>\n",
       "      <td>242.008264</td>\n",
       "      <td>176.045449</td>\n",
       "      <td>239.002092</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>97.020616</td>\n",
       "      <td>18.027756</td>\n",
       "      <td>62.008064</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26.453720</td>\n",
       "      <td>1</td>\n",
       "      <td>img_1.bmp</td>\n",
       "      <td>227.140925</td>\n",
       "      <td>185.067555</td>\n",
       "      <td>220.056811</td>\n",
       "      <td>48.041649</td>\n",
       "      <td>88.051122</td>\n",
       "      <td>22.022716</td>\n",
       "      <td>59.008474</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34.967561</td>\n",
       "      <td>1</td>\n",
       "      <td>img_2.bmp</td>\n",
       "      <td>269.185809</td>\n",
       "      <td>217.057596</td>\n",
       "      <td>266.030073</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>103.019416</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>59.033889</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22.044766</td>\n",
       "      <td>1</td>\n",
       "      <td>img_3.bmp</td>\n",
       "      <td>244.018442</td>\n",
       "      <td>216.148097</td>\n",
       "      <td>239.002092</td>\n",
       "      <td>44.045431</td>\n",
       "      <td>102.044108</td>\n",
       "      <td>50.039984</td>\n",
       "      <td>59.008474</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25.845588</td>\n",
       "      <td>1</td>\n",
       "      <td>img_6.bmp</td>\n",
       "      <td>238.838858</td>\n",
       "      <td>196.063765</td>\n",
       "      <td>236.541751</td>\n",
       "      <td>48.041649</td>\n",
       "      <td>93.085982</td>\n",
       "      <td>24.020824</td>\n",
       "      <td>69.260378</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         bmi  is_training       name   jaw_width  face_height  \\\n",
       "0  34.207396            1  img_0.bmp  242.008264   176.045449   \n",
       "1  26.453720            1  img_1.bmp  227.140925   185.067555   \n",
       "2  34.967561            1  img_2.bmp  269.185809   217.057596   \n",
       "3  22.044766            1  img_3.bmp  244.018442   216.148097   \n",
       "4  25.845588            1  img_6.bmp  238.838858   196.063765   \n",
       "\n",
       "   cheekbone_width  nose_width  mouth_width  mouth_height  eye_distance  ...  \\\n",
       "0       239.002092   50.000000    97.020616     18.027756     62.008064  ...   \n",
       "1       220.056811   48.041649    88.051122     22.022716     59.008474  ...   \n",
       "2       266.030073   48.000000   103.019416     51.000000     59.033889  ...   \n",
       "3       239.002092   44.045431   102.044108     50.039984     59.008474  ...   \n",
       "4       236.541751   48.041649    93.085982     24.020824     69.260378  ...   \n",
       "\n",
       "   race_Latino_Hispanic  race_White  gender_Female  gender_Male  age_10-19  \\\n",
       "0                     0           1              0            1          0   \n",
       "1                     0           1              0            1          0   \n",
       "2                     0           1              1            0          0   \n",
       "3                     0           1              1            0          0   \n",
       "4                     0           1              1            0          1   \n",
       "\n",
       "   age_20-29  age_3-9  age_30-39  age_40-49  age_50-59  \n",
       "0          0        0          1          0          0  \n",
       "1          1        0          0          0          0  \n",
       "2          1        0          0          0          0  \n",
       "3          1        0          0          0          0  \n",
       "4          0        0          0          0          0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### One Hot Encode ###\n",
    "race_dummies = pd.get_dummies(df['race'], prefix='race').astype(int)\n",
    "gender_dummies = pd.get_dummies(df['pred_gender'], prefix='gender').astype(int)\n",
    "age_dummies = pd.get_dummies(df['age'], prefix='age').astype(int)\n",
    "\n",
    "# Fix column names immediately after get_dummies\n",
    "race_dummies.columns = [col.replace(\" \", \"_\") for col in race_dummies.columns]\n",
    "gender_dummies.columns = [col.replace(\" \", \"_\") for col in gender_dummies.columns]\n",
    "age_dummies.columns = [col.replace(\" \", \"_\") for col in age_dummies.columns]\n",
    "\n",
    "# Combine with original numeric features\n",
    "df_final = pd.concat([\n",
    "    df.drop(['race', 'pred_gender', 'age'], axis=1),\n",
    "    race_dummies,\n",
    "    gender_dummies,\n",
    "    age_dummies\n",
    "], axis=1)\n",
    "\n",
    "\n",
    "del df, race_dummies, gender_dummies, age_dummies\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['bmi', 'is_training', 'name', 'jaw_width', 'face_height',\n",
       "       'cheekbone_width', 'nose_width', 'mouth_width', 'mouth_height',\n",
       "       'eye_distance', 'left_eye_width', 'right_eye_width',\n",
       "       'face_width_to_height', 'mouth_to_nose_ratio', 'bmi_norm', 'race_Black',\n",
       "       'race_East_Asian', 'race_Latino_Hispanic', 'race_White',\n",
       "       'gender_Female', 'gender_Male', 'age_10-19', 'age_20-29', 'age_3-9',\n",
       "       'age_30-39', 'age_40-49', 'age_50-59'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: object)\n"
     ]
    }
   ],
   "source": [
    "extra_feature_cols = [col for col in df_final.columns if col not in ['name', 'bmi', 'bmi_norm' 'is_training']]\n",
    "print(df_final[extra_feature_cols].dtypes[df_final[extra_feature_cols].dtypes == \"object\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Detection & Alignment using MTCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will use saved embeddings from prior process\n"
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# STEP X: Precompute and Cache Face Embeddings\n",
    "# ===============================================\n",
    "from facenet_pytorch import InceptionResnetV1, MTCNN\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "IMAGE_DIR = \"data/Images\"\n",
    "EMBEDDING_CACHE_DIR = \"data/embeddings\"\n",
    "os.makedirs(EMBEDDING_CACHE_DIR, exist_ok=True)\n",
    "\n",
    "# mtcnn = MTCNN(image_size=160, margin=20, post_process=True, device=device)\n",
    "mtcnn = MTCNN(image_size=224, margin=20, post_process=True, device=device)\n",
    "embedder = InceptionResnetV1(pretrained=\"vggface2\").eval().to(device)\n",
    "\n",
    "def extract_face_embedding(image_path):\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    face = mtcnn(img)\n",
    "    if face is None:\n",
    "        # return None\n",
    "        return torch.zeros(512).numpy()\n",
    "    face = face.unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        embedding = embedder(face).squeeze(0).cpu().numpy()\n",
    "    return embedding\n",
    "\n",
    "generate_emb = input(\"Generate embeddings?: Y or N\").lower().strip() or \"n\"\n",
    "\n",
    "if generate_emb == 'y':\n",
    "    # Generate embeddings\n",
    "    for idx, row in tqdm(df_final.iterrows(), total=len(df_final), desc=\"Caching embeddings\"):\n",
    "        image_name = row[\"name\"]\n",
    "        image_path = os.path.join(IMAGE_DIR, image_name)\n",
    "        save_path = os.path.join(EMBEDDING_CACHE_DIR, image_name.replace(\".bmp\", \".npy\"))\n",
    "\n",
    "        # if not os.path.exists(save_path):\n",
    "        try:\n",
    "            embedding = extract_face_embedding(image_path)\n",
    "            if embedding is not None:\n",
    "                np.save(save_path, embedding)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed {image_name}: {e}\")\n",
    "else: \n",
    "    print(\"Will use saved embeddings from prior process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3712, 27)\n"
     ]
    }
   ],
   "source": [
    "# Remove images we were not able to create embeddings \n",
    "from pathlib import Path\n",
    "df_final = df_final[df_final[\"name\"].apply(lambda x: Path(\"data/embeddings\") / x.replace(\".bmp\", \".npy\")).apply(Path.exists)].reset_index(drop=True)\n",
    "print(df_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms as T\n",
    "\n",
    "image_transforms = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "class BMIDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, embedding_dir=\"data/embeddings\", transform=None):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.image_dir = image_dir\n",
    "        self.embedding_dir = embedding_dir\n",
    "        self.transform = transform\n",
    "        self.device = device\n",
    "\n",
    "        self.mtcnn = MTCNN(image_size=224, margin=20, post_process=True, device=self.device)\n",
    "        self.embedder = InceptionResnetV1(pretrained='vggface2').eval().to(self.device)\n",
    "\n",
    "\n",
    "        # Identify tabular feature columns\n",
    "        self.extra_feature_cols = [\n",
    "            col for col in dataframe.columns\n",
    "            if col not in ['name', 'bmi', 'bmi_norm', 'is_training']\n",
    "        ]\n",
    "        self.scaler = StandardScaler()\n",
    "        self.df[self.extra_feature_cols] = self.scaler.fit_transform(self.df[self.extra_feature_cols])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_path = os.path.join(self.image_dir, row['name'])\n",
    "\n",
    "        img = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        # Face alignment\n",
    "        face = self.mtcnn(img)\n",
    "        if face is None:\n",
    "            face_embedding = torch.zeros(512)\n",
    "        else:\n",
    "            face = face.unsqueeze(0).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                face_embedding = self.embedder(face).squeeze(0).cpu()\n",
    "\n",
    "        # extra_features = torch.tensor(row[self.extra_feature_cols].values, dtype=torch.float32)\n",
    "        extra_features = torch.tensor(row[self.extra_feature_cols].values.astype(np.float32), dtype=torch.float32)\n",
    "        bmi = torch.tensor(row['bmi_norm'], dtype=torch.float32)\n",
    "\n",
    "        return face_embedding, extra_features, bmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# STEP 5: Model Definition (Multitask)\n",
    "# ===============================================\n",
    "class BMIMultitaskModel(nn.Module):\n",
    "    def __init__(self, extra_feature_dim):\n",
    "        super().__init__()\n",
    "        self.backbone = None  # no longer needed with cached embeddings\n",
    "\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(512 + extra_feature_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "        )\n",
    "\n",
    "        self.bmi_head = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "        self.age_head = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "        self.gender_head = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)  # or 2 if using softmax\n",
    "        )\n",
    "\n",
    "    def forward(self, face_embed, extra_features):\n",
    "        x = torch.cat([face_embed, extra_features], dim=1)\n",
    "        x = self.shared(x)\n",
    "\n",
    "        bmi = self.bmi_head(x).squeeze(1)\n",
    "        age = self.age_head(x).squeeze(1)\n",
    "        gender = self.gender_head(x).squeeze(1)  # sigmoid if binary\n",
    "\n",
    "        return bmi, age, gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (2105, 27)\n",
      "Validation set shape: (903, 27)\n",
      "Number of Males in training set shape: 1241; 58.95%\n",
      "Number of Females in training set shape: 864; 41.05%\n",
      "Number of Males in validation set shape: 534; 59.14%\n",
      "Number of Females in validation set shape: 369; 40.86%\n"
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# STEP 6: Prepare Data Loaders\n",
    "# ===============================================\n",
    "\n",
    "# Obtain values to normalize target variable\n",
    "\n",
    "X_train = df_final[df_final['is_training'] == 1]\n",
    "test_df = df_final[df_final['is_training'] == 0]\n",
    "\n",
    "# Create validation set\n",
    "validation_set_percent = 0.3\n",
    "\n",
    "train_df, validation_df = train_test_split(X_train, test_size=0.3, random_state=42)\n",
    "\n",
    "print(f\"Training set shape: {train_df.shape}\")\n",
    "print(f\"Validation set shape: {validation_df.shape}\")\n",
    "print(f\"Number of Males in training set shape: {train_df['gender_Male'].sum()}; {train_df['gender_Male'].sum()/len(train_df) * 100:.2f}%\")\n",
    "print(f\"Number of Females in training set shape: {train_df['gender_Female'].sum()}; {train_df['gender_Female'].sum()/len(train_df) * 100:.2f}%\")\n",
    "print(f\"Number of Males in validation set shape: {validation_df['gender_Male'].sum()}; {validation_df['gender_Male'].sum()/len(validation_df) * 100:.2f}%\")\n",
    "print(f\"Number of Females in validation set shape: {validation_df['gender_Female'].sum()}; {validation_df['gender_Female'].sum()/len(validation_df) * 100:.2f}%\")\n",
    "\n",
    "train_dataset = BMIDataset(train_df, image_dir=IMAGE_DIR)\n",
    "val_dataset = BMIDataset(validation_df, image_dir=IMAGE_DIR)\n",
    "test_dataset = BMIDataset(test_df, image_dir=IMAGE_DIR)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "validation_loader = DataLoader(val_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure paths for best model\n",
    "MODEL_PATH = \"saved_models/best_bmi_prediction_model.pth\"\n",
    "os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# STEP 7: Training Loop\n",
    "# ===============================================\n",
    "\n",
    "def evaluate(model, criterion, dataloader):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for face, extra, bmi in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            face = face.to(DEVICE, non_blocking=True)\n",
    "            extra = extra.to(DEVICE, non_blocking=True)\n",
    "            bmi = bmi.to(DEVICE, non_blocking=True)\n",
    "\n",
    "            pred_bmi, _, _ = model(face, extra)\n",
    "            loss = criterion(pred_bmi, bmi)\n",
    "            val_loss += loss.item()\n",
    "            all_preds.append(pred_bmi.cpu().numpy())\n",
    "            all_labels.append(bmi.cpu().numpy())\n",
    "    \n",
    "    avg_val_loss = val_loss / len(dataloader)\n",
    "    \n",
    "    preds = np.concatenate(all_preds)\n",
    "    labels = np.concatenate(all_labels)\n",
    "    \n",
    "    # Reverse transformation from normalized bmi values\n",
    "    preds = preds * bmi_std + bmi_mean\n",
    "    labels = labels * bmi_std + bmi_mean\n",
    "    \n",
    "    # Evaluate performance\n",
    "    mae = mean_absolute_error(labels, preds)\n",
    "    r2 = r2_score(labels, preds)\n",
    "    pearson_corr, _ = pearsonr(labels, preds)\n",
    "\n",
    "    print(f\"Evaluation: MAE={mae:.2f}, R²={r2:.3f}, Pearson r={pearson_corr:.3f}\")\n",
    "    return avg_val_loss, mae, r2, pearson_corr\n",
    "\n",
    "def train_epoch(model, optimizer, criterion, dataloader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for face, extra, bmi in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        # face, extra, bmi = face.to(DEVICE), extra.to(DEVICE), bmi.to(DEVICE)\n",
    "        face = face.to(DEVICE, non_blocking=True)\n",
    "        extra = extra.to(DEVICE, non_blocking=True)\n",
    "        bmi = bmi.to(DEVICE, non_blocking=True)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        pred_bmi, _, _ = model(face, extra)  # Only use BMI for loss\n",
    "        loss = criterion(pred_bmi, bmi)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def train_model(model, optimizer, criterion, epochs=150):\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        loss = train_epoch(model, optimizer, criterion, train_loader)\n",
    "        avg_val_loss, val_mae, r2_value, pearson_corr = evaluate(model, criterion, validation_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}]: Train Loss={loss:.4f}, Val Loss={avg_val_loss:.4f}, Val MAE={val_mae:.3f}, Val R^2={r2_value:.3f}, Val Pearson Coefficient={pearson_corr:.3f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            print(f\"Prior best: {best_val_loss}\")\n",
    "            best_val_loss = avg_val_loss\n",
    "            print(f\"Current best: {best_val_loss}\")\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            print(f\"Best model saved to: {MODEL_PATH}\")\n",
    "\n",
    "def load_best_model(extra_feature_dim):\n",
    "    model = BMIMultitaskModel(extra_feature_dim=extra_feature_dim).to(DEVICE)\n",
    "    checkpoint = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: MAE=6.35, R²=0.047, Pearson r=0.490\n",
      "Epoch [1/150]: Train Loss=0.9173, Val Loss=0.9593, Val MAE=6.349, Val R^2=0.047, Val Pearson Coefficient=0.490\n",
      "Prior best: inf\n",
      "Current best: 0.9593148745339493\n",
      "Best model saved to: saved_models/best_bmi_prediction_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: MAE=6.05, R²=0.132, Pearson r=0.511\n",
      "Epoch [2/150]: Train Loss=0.8570, Val Loss=0.8710, Val MAE=6.046, Val R^2=0.132, Val Pearson Coefficient=0.511\n",
      "Prior best: 0.9593148745339493\n",
      "Current best: 0.8709916143581785\n",
      "Best model saved to: saved_models/best_bmi_prediction_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: MAE=5.67, R²=0.234, Pearson r=0.536\n",
      "Epoch [3/150]: Train Loss=0.7674, Val Loss=0.7734, Val MAE=5.668, Val R^2=0.234, Val Pearson Coefficient=0.536\n",
      "Prior best: 0.8709916143581785\n",
      "Current best: 0.7733697809022049\n",
      "Best model saved to: saved_models/best_bmi_prediction_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: MAE=5.33, R²=0.299, Pearson r=0.560\n",
      "Epoch [4/150]: Train Loss=0.6879, Val Loss=0.7051, Val MAE=5.328, Val R^2=0.299, Val Pearson Coefficient=0.560\n",
      "Prior best: 0.7733697809022049\n",
      "Current best: 0.7051490298632918\n",
      "Best model saved to: saved_models/best_bmi_prediction_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: MAE=5.15, R²=0.337, Pearson r=0.585\n",
      "Epoch [5/150]: Train Loss=0.6360, Val Loss=0.6590, Val MAE=5.147, Val R^2=0.337, Val Pearson Coefficient=0.585\n",
      "Prior best: 0.7051490298632918\n",
      "Current best: 0.6590387235427725\n",
      "Best model saved to: saved_models/best_bmi_prediction_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: MAE=4.99, R²=0.362, Pearson r=0.605\n",
      "Epoch [6/150]: Train Loss=0.6048, Val Loss=0.6425, Val MAE=4.992, Val R^2=0.362, Val Pearson Coefficient=0.605\n",
      "Prior best: 0.6590387235427725\n",
      "Current best: 0.6424722589295486\n",
      "Best model saved to: saved_models/best_bmi_prediction_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: MAE=4.92, R²=0.384, Pearson r=0.621\n",
      "Epoch [7/150]: Train Loss=0.5839, Val Loss=0.6229, Val MAE=4.924, Val R^2=0.384, Val Pearson Coefficient=0.621\n",
      "Prior best: 0.6424722589295486\n",
      "Current best: 0.6229321771654589\n",
      "Best model saved to: saved_models/best_bmi_prediction_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: MAE=4.85, R²=0.395, Pearson r=0.631\n",
      "Epoch [8/150]: Train Loss=0.5656, Val Loss=0.6133, Val MAE=4.849, Val R^2=0.395, Val Pearson Coefficient=0.631\n",
      "Prior best: 0.6229321771654589\n",
      "Current best: 0.6132939098210171\n",
      "Best model saved to: saved_models/best_bmi_prediction_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: MAE=4.81, R²=0.410, Pearson r=0.643\n",
      "Epoch [9/150]: Train Loss=0.5506, Val Loss=0.6063, Val MAE=4.813, Val R^2=0.410, Val Pearson Coefficient=0.643\n",
      "Prior best: 0.6132939098210171\n",
      "Current best: 0.6062649745365669\n",
      "Best model saved to: saved_models/best_bmi_prediction_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: MAE=4.77, R²=0.416, Pearson r=0.646\n",
      "Epoch [10/150]: Train Loss=0.5397, Val Loss=0.5935, Val MAE=4.773, Val R^2=0.416, Val Pearson Coefficient=0.646\n",
      "Prior best: 0.6062649745365669\n",
      "Current best: 0.5935370243828872\n",
      "Best model saved to: saved_models/best_bmi_prediction_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_train_model \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBeginning training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m     \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Complete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[32], line 60\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, criterion, epochs)\u001b[0m\n\u001b[1;32m     57\u001b[0m best_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 60\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     avg_val_loss, val_mae, r2_value, pearson_corr \u001b[38;5;241m=\u001b[39m evaluate(model, criterion, validation_loader)\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]: Train Loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val Loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_val_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val MAE=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_mae\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val R^2=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr2_value\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val Pearson Coefficient=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpearson_corr\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[32], line 41\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, optimizer, criterion, dataloader)\u001b[0m\n\u001b[1;32m     39\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     40\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m face, extra, bmi \u001b[38;5;129;01min\u001b[39;00m tqdm(dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# face, extra, bmi = face.to(DEVICE), extra.to(DEVICE), bmi.to(DEVICE)\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     face \u001b[38;5;241m=\u001b[39m face\u001b[38;5;241m.\u001b[39mto(DEVICE, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     44\u001b[0m     extra \u001b[38;5;241m=\u001b[39m extra\u001b[38;5;241m.\u001b[39mto(DEVICE, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniforge3/envs/bmi-predictor/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/bmi-predictor/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniforge3/envs/bmi-predictor/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniforge3/envs/bmi-predictor/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniforge3/envs/bmi-predictor/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[28], line 42\u001b[0m, in \u001b[0;36mBMIDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     39\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Face alignment\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m face \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmtcnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m face \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     face_embedding \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m512\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/bmi-predictor/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/bmi-predictor/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/bmi-predictor/lib/python3.10/site-packages/facenet_pytorch/models/mtcnn.py:258\u001b[0m, in \u001b[0;36mMTCNN.forward\u001b[0;34m(self, img, save_path, return_prob)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run MTCNN face detection on a PIL image or numpy array. This method performs both\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03mdetection and extraction of faces, returning tensors representing detected faces rather\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03mthan the bounding boxes. To access bounding boxes, see the MTCNN.detect() method below.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;124;03m>>> face_tensor, prob = mtcnn(img, save_path='face.png', return_prob=True)\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# Detect faces\u001b[39;00m\n\u001b[0;32m--> 258\u001b[0m batch_boxes, batch_probs, batch_points \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlandmarks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# Select faces\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_all:\n",
      "File \u001b[0;32m~/miniforge3/envs/bmi-predictor/lib/python3.10/site-packages/facenet_pytorch/models/mtcnn.py:313\u001b[0m, in \u001b[0;36mMTCNN.detect\u001b[0;34m(self, img, landmarks)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Detect all faces in PIL image and return bounding boxes and optional facial landmarks.\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03mThis method is used by the forward method and is also useful for face detection tasks\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m>>> img_draw.save('annotated_faces.png')\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 313\u001b[0m     batch_boxes, batch_points \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_face\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_face_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthresholds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfactor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m boxes, probs, points \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m box, point \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batch_boxes, batch_points):\n",
      "File \u001b[0;32m~/miniforge3/envs/bmi-predictor/lib/python3.10/site-packages/facenet_pytorch/models/utils/detect_face.py:112\u001b[0m, in \u001b[0;36mdetect_face\u001b[0;34m(imgs, minsize, pnet, rnet, onet, threshold, factor, device)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ey[k] \u001b[38;5;241m>\u001b[39m (y[k] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m ex[k] \u001b[38;5;241m>\u001b[39m (x[k] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    111\u001b[0m         img_k \u001b[38;5;241m=\u001b[39m imgs[image_inds[k], :, (y[k] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):ey[k], (x[k] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):ex[k]]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 112\u001b[0m         im_data\u001b[38;5;241m.\u001b[39mappend(\u001b[43mimresample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    113\u001b[0m im_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(im_data, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    114\u001b[0m im_data \u001b[38;5;241m=\u001b[39m (im_data \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m127.5\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.0078125\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/bmi-predictor/lib/python3.10/site-packages/facenet_pytorch/models/utils/detect_face.py:305\u001b[0m, in \u001b[0;36mimresample\u001b[0;34m(img, sz)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mimresample\u001b[39m(img, sz):\n\u001b[0;32m--> 305\u001b[0m     im_data \u001b[38;5;241m=\u001b[39m \u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marea\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m im_data\n",
      "File \u001b[0;32m~/miniforge3/envs/bmi-predictor/lib/python3.10/site-packages/torch/nn/functional.py:4017\u001b[0m, in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   4015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marea\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   4016\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m output_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 4017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43madaptive_avg_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4018\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marea\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   4019\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m output_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/bmi-predictor/lib/python3.10/site-packages/torch/nn/functional.py:1233\u001b[0m, in \u001b[0;36madaptive_avg_pool2d\u001b[0;34m(input, output_size)\u001b[0m\n\u001b[1;32m   1231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(adaptive_avg_pool2d, (\u001b[38;5;28minput\u001b[39m,), \u001b[38;5;28minput\u001b[39m, output_size)\n\u001b[1;32m   1232\u001b[0m _output_size \u001b[38;5;241m=\u001b[39m _list_with_default(output_size, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 1233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madaptive_avg_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_output_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "is_train_model = input(\"Train new model? Y or N\\n Default: Use best saved model\").lower().strip() or \"n\"\n",
    "\n",
    "model = BMIMultitaskModel(extra_feature_dim=len(train_dataset.extra_feature_cols)).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "if is_train_model == 'y':\n",
    "    print(\"Beginning training\")\n",
    "    train_model(model, optimizer, criterion)\n",
    "    print(\"Training Complete\")\n",
    "else:\n",
    "    print(\"Loading saved best model\")\n",
    "# Get extra features dimension\n",
    "extra_feature_dim = len(test_dataset.extra_feature_cols)\n",
    "best_model = load_best_model(extra_feature_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results from testing set \n",
    "model = BMIMultitaskModel(extra_feature_dim=len(train_dataset.extra_feature_cols)).to(DEVICE)\n",
    "checkpoint = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "model.load_state_dict(checkpoint)\n",
    "model.eval()\n",
    "_, test_mae, test_r2, test_pearson = evaluate(model, criterion, test_loader)\n",
    "print(f\"\\nTesting Set MAE={test_mae:.3f}, R²={test_r2:.3f}, Pearson Coefficient={test_pearson:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# from PIL import Image\n",
    "# import os\n",
    "\n",
    "# ===============================================\n",
    "# Load a single image and predict BMI\n",
    "# ===============================================\n",
    "def predict_outputs(image_name, model, extra_feature_df, embedding_dir=\"data/embeddings\"):\n",
    "    model.eval()\n",
    "    \n",
    "    embedding_path = os.path.join(embedding_dir, image_name.replace(\".bmp\", \".npy\"))\n",
    "    if not os.path.exists(embedding_path):\n",
    "        raise FileNotFoundError(f\"Embedding not found for: {image_name}\")\n",
    "\n",
    "    face_tensor = torch.tensor(np.load(embedding_path), dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    row = extra_feature_df[extra_feature_df[\"name\"] == image_name]\n",
    "    if row.empty:\n",
    "        raise ValueError(f\"No extra feature data for {image_name}\")\n",
    "\n",
    "    extra_cols = train_dataset.extra_feature_cols\n",
    "    extra_features = torch.tensor(row[extra_cols].values.astype(np.float32), dtype=torch.float32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_bmi, pred_age, pred_gender_logits = model(face_tensor, extra_features)\n",
    "        pred_gender = \"Male\" if torch.sigmoid(pred_gender_logits).item() > 0.5 else \"Female\"\n",
    "\n",
    "    pred_bmi = pred_bmi.item() * bmi_std + bmi_mean\n",
    "    \n",
    "    return pred_bmi, pred_age.item(), pred_gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = \"img_2.bmp\" \n",
    "\n",
    "pred_bmi, _, pred_gender = predict_outputs(test_image, model, df_final)\n",
    "actual_bmi = df_final[df_final['name'] == test_image]['bmi'].iloc[0]\n",
    "print(f\"Predicted BMI: {pred_bmi:.2f}, Gender: {pred_gender}\")\n",
    "print(f\"Actual BMI for {test_image}: {actual_bmi:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
