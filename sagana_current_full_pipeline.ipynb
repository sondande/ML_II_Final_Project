{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Full Pipeline For BMI detection using extra features extracted as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# conda create -n bmi-predictor python=3.10 -y\n",
    "# conda activate bmi-predictor\n",
    "\n",
    "# # For MPS on macOS with ARM (M1/M2/M3)\n",
    "# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cpu\n",
    "\n",
    "# pip install facenet-pytorch\n",
    "# pip install pandas scikit-learn matplotlib tqdm jupyter notebook\n",
    "# pip install opencv-python pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core count: 10\n",
      "Using device: cpu\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# STEP 0: Imports and Setup\n",
    "# ===============================================\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from facenet_pytorch import InceptionResnetV1, MTCNN\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm \n",
    "\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Core count: {os.cpu_count()}')\n",
    "torch.set_num_threads(os.cpu_count())  # Use all available CPU cores\n",
    "DEVICE = torch.device(\"cpu\")  # Force CPU usage for portability\n",
    "print(\"Using device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4206 rows from CSV\n",
      "Filtered to 3962 rows with existing images\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bmi</th>\n",
       "      <th>gender</th>\n",
       "      <th>is_training</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.207396</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>img_0.bmp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26.453720</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>img_1.bmp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34.967561</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "      <td>img_2.bmp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22.044766</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "      <td>img_3.bmp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>25.845588</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "      <td>img_6.bmp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         bmi  gender  is_training       name\n",
       "0  34.207396    Male            1  img_0.bmp\n",
       "1  26.453720    Male            1  img_1.bmp\n",
       "2  34.967561  Female            1  img_2.bmp\n",
       "3  22.044766  Female            1  img_3.bmp\n",
       "6  25.845588  Female            1  img_6.bmp"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paths\n",
    "IMAGE_DIR = \"data/Images\"\n",
    "existing_image_files = os.listdir(IMAGE_DIR)\n",
    "\n",
    "# Load CSV\n",
    "data_df = pd.read_csv(\"data/data.csv\").drop(columns=[\"Unnamed: 0\"])\n",
    "print(f\"Loaded {data_df.shape[0]} rows from CSV\")\n",
    "\n",
    "# Filter out rows with missing image files\n",
    "data_df = data_df[data_df[\"name\"].isin(existing_image_files)]\n",
    "print(f\"Filtered to {data_df.shape[0]} rows with existing images\")\n",
    "\n",
    "del existing_image_files\n",
    "data_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jaw_width</th>\n",
       "      <th>face_height</th>\n",
       "      <th>cheekbone_width</th>\n",
       "      <th>nose_width</th>\n",
       "      <th>mouth_width</th>\n",
       "      <th>mouth_height</th>\n",
       "      <th>eye_distance</th>\n",
       "      <th>left_eye_width</th>\n",
       "      <th>right_eye_width</th>\n",
       "      <th>face_width_to_height</th>\n",
       "      <th>mouth_to_nose_ratio</th>\n",
       "      <th>image</th>\n",
       "      <th>race</th>\n",
       "      <th>pred_gender</th>\n",
       "      <th>age</th>\n",
       "      <th>key</th>\n",
       "      <th>bmi</th>\n",
       "      <th>gender</th>\n",
       "      <th>is_training</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>237.002110</td>\n",
       "      <td>210.287898</td>\n",
       "      <td>237.008439</td>\n",
       "      <td>52.009614</td>\n",
       "      <td>120.016666</td>\n",
       "      <td>58.034473</td>\n",
       "      <td>62.008064</td>\n",
       "      <td>46.173586</td>\n",
       "      <td>42.107007</td>\n",
       "      <td>1.127036</td>\n",
       "      <td>2.307586</td>\n",
       "      <td>img_1066_face0.jpg</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>20-29</td>\n",
       "      <td>img_1066</td>\n",
       "      <td>41.191406</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "      <td>img_1066.bmp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>226.035395</td>\n",
       "      <td>200.010000</td>\n",
       "      <td>220.002273</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>104.076895</td>\n",
       "      <td>38.052595</td>\n",
       "      <td>62.072538</td>\n",
       "      <td>43.011626</td>\n",
       "      <td>41.048752</td>\n",
       "      <td>1.130120</td>\n",
       "      <td>2.040723</td>\n",
       "      <td>img_1585_face0.jpg</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>20-29</td>\n",
       "      <td>img_1585</td>\n",
       "      <td>24.658895</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>img_1585.bmp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>241.101638</td>\n",
       "      <td>209.193690</td>\n",
       "      <td>239.075302</td>\n",
       "      <td>42.011903</td>\n",
       "      <td>87.005747</td>\n",
       "      <td>32.062439</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>42.107007</td>\n",
       "      <td>43.011626</td>\n",
       "      <td>1.152528</td>\n",
       "      <td>2.070978</td>\n",
       "      <td>img_3852_face0.jpg</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>20-29</td>\n",
       "      <td>img_3852</td>\n",
       "      <td>39.151259</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>img_3852.bmp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>229.490741</td>\n",
       "      <td>198.494332</td>\n",
       "      <td>229.176788</td>\n",
       "      <td>53.037722</td>\n",
       "      <td>80.056230</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>54.083269</td>\n",
       "      <td>42.011903</td>\n",
       "      <td>41.012193</td>\n",
       "      <td>1.156158</td>\n",
       "      <td>1.509421</td>\n",
       "      <td>img_1857_face0.jpg</td>\n",
       "      <td>East Asian</td>\n",
       "      <td>Male</td>\n",
       "      <td>30-39</td>\n",
       "      <td>img_1857</td>\n",
       "      <td>25.845588</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>img_1857.bmp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>235.552542</td>\n",
       "      <td>201.613492</td>\n",
       "      <td>235.766834</td>\n",
       "      <td>48.041649</td>\n",
       "      <td>77.524190</td>\n",
       "      <td>27.892651</td>\n",
       "      <td>60.299254</td>\n",
       "      <td>34.014703</td>\n",
       "      <td>41.194660</td>\n",
       "      <td>1.168337</td>\n",
       "      <td>1.613687</td>\n",
       "      <td>img_4196_face0.jpg</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>20-29</td>\n",
       "      <td>img_4196</td>\n",
       "      <td>36.243556</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>img_4196.bmp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    jaw_width  face_height  cheekbone_width  nose_width  mouth_width  \\\n",
       "0  237.002110   210.287898       237.008439   52.009614   120.016666   \n",
       "1  226.035395   200.010000       220.002273   51.000000   104.076895   \n",
       "2  241.101638   209.193690       239.075302   42.011903    87.005747   \n",
       "3  229.490741   198.494332       229.176788   53.037722    80.056230   \n",
       "4  235.552542   201.613492       235.766834   48.041649    77.524190   \n",
       "\n",
       "   mouth_height  eye_distance  left_eye_width  right_eye_width  \\\n",
       "0     58.034473     62.008064       46.173586        42.107007   \n",
       "1     38.052595     62.072538       43.011626        41.048752   \n",
       "2     32.062439     65.000000       42.107007        43.011626   \n",
       "3     12.000000     54.083269       42.011903        41.012193   \n",
       "4     27.892651     60.299254       34.014703        41.194660   \n",
       "\n",
       "   face_width_to_height  mouth_to_nose_ratio               image        race  \\\n",
       "0              1.127036             2.307586  img_1066_face0.jpg       White   \n",
       "1              1.130120             2.040723  img_1585_face0.jpg       White   \n",
       "2              1.152528             2.070978  img_3852_face0.jpg       White   \n",
       "3              1.156158             1.509421  img_1857_face0.jpg  East Asian   \n",
       "4              1.168337             1.613687  img_4196_face0.jpg       White   \n",
       "\n",
       "  pred_gender    age       key        bmi  gender  is_training          name  \n",
       "0      Female  20-29  img_1066  41.191406  Female            1  img_1066.bmp  \n",
       "1        Male  20-29  img_1585  24.658895    Male            1  img_1585.bmp  \n",
       "2      Female  20-29  img_3852  39.151259  Female            0  img_3852.bmp  \n",
       "3        Male  30-39  img_1857  25.845588    Male            1  img_1857.bmp  \n",
       "4        Male  20-29  img_4196  36.243556    Male            0  img_4196.bmp  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df = pd.read_csv('data/Feature_Add.csv')\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset shape: (3712, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bmi</th>\n",
       "      <th>is_training</th>\n",
       "      <th>name</th>\n",
       "      <th>jaw_width</th>\n",
       "      <th>face_height</th>\n",
       "      <th>cheekbone_width</th>\n",
       "      <th>nose_width</th>\n",
       "      <th>mouth_width</th>\n",
       "      <th>mouth_height</th>\n",
       "      <th>eye_distance</th>\n",
       "      <th>left_eye_width</th>\n",
       "      <th>right_eye_width</th>\n",
       "      <th>face_width_to_height</th>\n",
       "      <th>mouth_to_nose_ratio</th>\n",
       "      <th>race</th>\n",
       "      <th>pred_gender</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.207396</td>\n",
       "      <td>1</td>\n",
       "      <td>img_0.bmp</td>\n",
       "      <td>242.008264</td>\n",
       "      <td>176.045449</td>\n",
       "      <td>239.002092</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>97.020616</td>\n",
       "      <td>18.027756</td>\n",
       "      <td>62.008064</td>\n",
       "      <td>44.045431</td>\n",
       "      <td>43.046487</td>\n",
       "      <td>1.374692</td>\n",
       "      <td>1.940412</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>30-39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26.453720</td>\n",
       "      <td>1</td>\n",
       "      <td>img_1.bmp</td>\n",
       "      <td>227.140925</td>\n",
       "      <td>185.067555</td>\n",
       "      <td>220.056811</td>\n",
       "      <td>48.041649</td>\n",
       "      <td>88.051122</td>\n",
       "      <td>22.022716</td>\n",
       "      <td>59.008474</td>\n",
       "      <td>41.012193</td>\n",
       "      <td>43.185646</td>\n",
       "      <td>1.227341</td>\n",
       "      <td>1.832808</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>20-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34.967561</td>\n",
       "      <td>1</td>\n",
       "      <td>img_2.bmp</td>\n",
       "      <td>269.185809</td>\n",
       "      <td>217.057596</td>\n",
       "      <td>266.030073</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>103.019416</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>59.033889</td>\n",
       "      <td>42.011903</td>\n",
       "      <td>44.045431</td>\n",
       "      <td>1.240158</td>\n",
       "      <td>2.146238</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>20-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22.044766</td>\n",
       "      <td>1</td>\n",
       "      <td>img_3.bmp</td>\n",
       "      <td>244.018442</td>\n",
       "      <td>216.148097</td>\n",
       "      <td>239.002092</td>\n",
       "      <td>44.045431</td>\n",
       "      <td>102.044108</td>\n",
       "      <td>50.039984</td>\n",
       "      <td>59.008474</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>41.012193</td>\n",
       "      <td>1.128941</td>\n",
       "      <td>2.316792</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>20-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25.845588</td>\n",
       "      <td>1</td>\n",
       "      <td>img_6.bmp</td>\n",
       "      <td>238.838858</td>\n",
       "      <td>196.063765</td>\n",
       "      <td>236.541751</td>\n",
       "      <td>48.041649</td>\n",
       "      <td>93.085982</td>\n",
       "      <td>24.020824</td>\n",
       "      <td>69.260378</td>\n",
       "      <td>48.259714</td>\n",
       "      <td>47.010637</td>\n",
       "      <td>1.218169</td>\n",
       "      <td>1.937610</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>10-19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         bmi  is_training       name   jaw_width  face_height  \\\n",
       "0  34.207396            1  img_0.bmp  242.008264   176.045449   \n",
       "1  26.453720            1  img_1.bmp  227.140925   185.067555   \n",
       "2  34.967561            1  img_2.bmp  269.185809   217.057596   \n",
       "3  22.044766            1  img_3.bmp  244.018442   216.148097   \n",
       "4  25.845588            1  img_6.bmp  238.838858   196.063765   \n",
       "\n",
       "   cheekbone_width  nose_width  mouth_width  mouth_height  eye_distance  \\\n",
       "0       239.002092   50.000000    97.020616     18.027756     62.008064   \n",
       "1       220.056811   48.041649    88.051122     22.022716     59.008474   \n",
       "2       266.030073   48.000000   103.019416     51.000000     59.033889   \n",
       "3       239.002092   44.045431   102.044108     50.039984     59.008474   \n",
       "4       236.541751   48.041649    93.085982     24.020824     69.260378   \n",
       "\n",
       "   left_eye_width  right_eye_width  face_width_to_height  mouth_to_nose_ratio  \\\n",
       "0       44.045431        43.046487              1.374692             1.940412   \n",
       "1       41.012193        43.185646              1.227341             1.832808   \n",
       "2       42.011903        44.045431              1.240158             2.146238   \n",
       "3       41.000000        41.012193              1.128941             2.316792   \n",
       "4       48.259714        47.010637              1.218169             1.937610   \n",
       "\n",
       "    race pred_gender    age  \n",
       "0  White        Male  30-39  \n",
       "1  White        Male  20-29  \n",
       "2  White      Female  20-29  \n",
       "3  White      Female  20-29  \n",
       "4  White      Female  10-19  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.merge(data_df, features_df, on=['gender', 'bmi', 'name', 'is_training'], how='inner')\n",
    "df.drop(columns=['key', 'image', 'gender'], inplace=True)\n",
    "print(\"Combined dataset shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entire data shape: (3712, 17)\n",
      "Gender Distribution Over Entire Dataset:\n",
      "  pred_gender  percentage\n",
      "0        Male    0.581088\n",
      "1      Female    0.418912\n",
      "\n",
      "Training data shape: (3008, 17)\n",
      "Gender Distribution Over Training Dataset:\n",
      "  pred_gender  percentage\n",
      "0        Male    0.590093\n",
      "1      Female    0.409907\n",
      "\n",
      "Testing data shape: (704, 17)\n",
      "Gender Distribution Over Testing Dataset:\n",
      "  pred_gender  percentage\n",
      "0        Male    0.542614\n",
      "1      Female    0.457386\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### General information ####\n",
    "print(f\"Entire data shape: {df.shape}\")\n",
    "gender_total = df['pred_gender'].value_counts() / len(df)\n",
    "gender_total = gender_total.reset_index()\n",
    "gender_total.rename(columns={'count': 'percentage'}, inplace=True)\n",
    "print(f\"Gender Distribution Over Entire Dataset:\\n{gender_total}\\n\")\n",
    "\n",
    "# Split into training and testing sets\n",
    "training_data_df = df[df['is_training'] == 1]\n",
    "testing_data_df = df[df['is_training'] == 0]\n",
    "\n",
    "gender_training_data_df = training_data_df['pred_gender'].value_counts() / len(training_data_df)\n",
    "gender_training_data_df = gender_training_data_df.reset_index()\n",
    "gender_training_data_df.rename(columns={'count': 'percentage'}, inplace=True)\n",
    "print(f\"Training data shape: {training_data_df.shape}\")\n",
    "print(f\"Gender Distribution Over Training Dataset:\\n{gender_training_data_df}\\n\")\n",
    "\n",
    "gender_testing_data_df = testing_data_df['pred_gender'].value_counts() / len(testing_data_df)\n",
    "gender_testing_data_df = gender_testing_data_df.reset_index()\n",
    "gender_testing_data_df.rename(columns={'count': 'percentage'}, inplace=True)\n",
    "print(f\"Testing data shape: {testing_data_df.shape}\")\n",
    "print(f\"Gender Distribution Over Testing Dataset:\\n{gender_testing_data_df}\\n\")\n",
    "\n",
    "del data_df, features_df, gender_total, training_data_df, testing_data_df, gender_training_data_df, gender_testing_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bmi</th>\n",
       "      <th>is_training</th>\n",
       "      <th>name</th>\n",
       "      <th>jaw_width</th>\n",
       "      <th>face_height</th>\n",
       "      <th>cheekbone_width</th>\n",
       "      <th>nose_width</th>\n",
       "      <th>mouth_width</th>\n",
       "      <th>mouth_height</th>\n",
       "      <th>eye_distance</th>\n",
       "      <th>...</th>\n",
       "      <th>race_Latino_Hispanic</th>\n",
       "      <th>race_White</th>\n",
       "      <th>gender_Female</th>\n",
       "      <th>gender_Male</th>\n",
       "      <th>age_10-19</th>\n",
       "      <th>age_20-29</th>\n",
       "      <th>age_3-9</th>\n",
       "      <th>age_30-39</th>\n",
       "      <th>age_40-49</th>\n",
       "      <th>age_50-59</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.207396</td>\n",
       "      <td>1</td>\n",
       "      <td>img_0.bmp</td>\n",
       "      <td>242.008264</td>\n",
       "      <td>176.045449</td>\n",
       "      <td>239.002092</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>97.020616</td>\n",
       "      <td>18.027756</td>\n",
       "      <td>62.008064</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26.453720</td>\n",
       "      <td>1</td>\n",
       "      <td>img_1.bmp</td>\n",
       "      <td>227.140925</td>\n",
       "      <td>185.067555</td>\n",
       "      <td>220.056811</td>\n",
       "      <td>48.041649</td>\n",
       "      <td>88.051122</td>\n",
       "      <td>22.022716</td>\n",
       "      <td>59.008474</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34.967561</td>\n",
       "      <td>1</td>\n",
       "      <td>img_2.bmp</td>\n",
       "      <td>269.185809</td>\n",
       "      <td>217.057596</td>\n",
       "      <td>266.030073</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>103.019416</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>59.033889</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22.044766</td>\n",
       "      <td>1</td>\n",
       "      <td>img_3.bmp</td>\n",
       "      <td>244.018442</td>\n",
       "      <td>216.148097</td>\n",
       "      <td>239.002092</td>\n",
       "      <td>44.045431</td>\n",
       "      <td>102.044108</td>\n",
       "      <td>50.039984</td>\n",
       "      <td>59.008474</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25.845588</td>\n",
       "      <td>1</td>\n",
       "      <td>img_6.bmp</td>\n",
       "      <td>238.838858</td>\n",
       "      <td>196.063765</td>\n",
       "      <td>236.541751</td>\n",
       "      <td>48.041649</td>\n",
       "      <td>93.085982</td>\n",
       "      <td>24.020824</td>\n",
       "      <td>69.260378</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         bmi  is_training       name   jaw_width  face_height  \\\n",
       "0  34.207396            1  img_0.bmp  242.008264   176.045449   \n",
       "1  26.453720            1  img_1.bmp  227.140925   185.067555   \n",
       "2  34.967561            1  img_2.bmp  269.185809   217.057596   \n",
       "3  22.044766            1  img_3.bmp  244.018442   216.148097   \n",
       "4  25.845588            1  img_6.bmp  238.838858   196.063765   \n",
       "\n",
       "   cheekbone_width  nose_width  mouth_width  mouth_height  eye_distance  ...  \\\n",
       "0       239.002092   50.000000    97.020616     18.027756     62.008064  ...   \n",
       "1       220.056811   48.041649    88.051122     22.022716     59.008474  ...   \n",
       "2       266.030073   48.000000   103.019416     51.000000     59.033889  ...   \n",
       "3       239.002092   44.045431   102.044108     50.039984     59.008474  ...   \n",
       "4       236.541751   48.041649    93.085982     24.020824     69.260378  ...   \n",
       "\n",
       "   race_Latino_Hispanic  race_White  gender_Female  gender_Male  age_10-19  \\\n",
       "0                     0           1              0            1          0   \n",
       "1                     0           1              0            1          0   \n",
       "2                     0           1              1            0          0   \n",
       "3                     0           1              1            0          0   \n",
       "4                     0           1              1            0          1   \n",
       "\n",
       "   age_20-29  age_3-9  age_30-39  age_40-49  age_50-59  \n",
       "0          0        0          1          0          0  \n",
       "1          1        0          0          0          0  \n",
       "2          1        0          0          0          0  \n",
       "3          1        0          0          0          0  \n",
       "4          0        0          0          0          0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### One Hot Encode ###\n",
    "race_dummies = pd.get_dummies(df['race'], prefix='race').astype(int)\n",
    "gender_dummies = pd.get_dummies(df['pred_gender'], prefix='gender').astype(int)\n",
    "age_dummies = pd.get_dummies(df['age'], prefix='age').astype(int)\n",
    "\n",
    "# Fix column names immediately after get_dummies\n",
    "race_dummies.columns = [col.replace(\" \", \"_\") for col in race_dummies.columns]\n",
    "gender_dummies.columns = [col.replace(\" \", \"_\") for col in gender_dummies.columns]\n",
    "age_dummies.columns = [col.replace(\" \", \"_\") for col in age_dummies.columns]\n",
    "\n",
    "# Combine with original numeric features\n",
    "df_final = pd.concat([\n",
    "    df.drop(['race', 'pred_gender', 'age'], axis=1),\n",
    "    race_dummies,\n",
    "    gender_dummies,\n",
    "    age_dummies\n",
    "], axis=1)\n",
    "\n",
    "del df, race_dummies, gender_dummies, age_dummies\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: object)\n"
     ]
    }
   ],
   "source": [
    "extra_feature_cols = [col for col in df_final.columns if col not in ['name', 'bmi', 'is_training']]\n",
    "print(df_final[extra_feature_cols].dtypes[df_final[extra_feature_cols].dtypes == \"object\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Detection & Alignment using MTCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# STEP 2: MTCNN for Face Cropping\n",
    "# ===============================================\n",
    "# Add margin for help to capture facial shape and surrounding features\n",
    "# post_process to True to have MTCNN handle resizing and normalization \n",
    "mtcnn = MTCNN(image_size=160, margin=20, post_process=True, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# STEP 4: Dataset Class\n",
    "# ===============================================\n",
    "class BMIDataset(Dataset):\n",
    "    def __init__(self, df, image_dir):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.image_dir = Path(image_dir)\n",
    "        \n",
    "        # Numerical + one-hot encoded feature columns\n",
    "        self.extra_feature_cols = [col for col in df.columns if col not in ['name', 'bmi', 'is_training']]\n",
    "        self.scaler = StandardScaler()\n",
    "        self.df[self.extra_feature_cols] = self.df[self.extra_feature_cols].apply(pd.to_numeric, errors='coerce')\n",
    "        self.df[self.extra_feature_cols] = self.df[self.extra_feature_cols].fillna(0.0)\n",
    "        self.df[self.extra_feature_cols] = self.scaler.fit_transform(self.df[self.extra_feature_cols])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_path = self.image_dir / row['name']\n",
    "\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        face = mtcnn(image)\n",
    "        \n",
    "        if face is None:\n",
    "            face = torch.zeros((3, 160, 160))  # handle missing face\n",
    "\n",
    "        # Ensure extra_features is a tensor of float32\n",
    "        extra_features = torch.tensor(\n",
    "            np.array(row[self.extra_feature_cols].values, dtype=np.float32)\n",
    "        )\n",
    "        bmi = torch.tensor(row['bmi'], dtype=torch.float32)\n",
    "\n",
    "        return face, extra_features, bmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# STEP 5: Model Definition (Multitask)\n",
    "# ===============================================\n",
    "class BMIMultitaskModel(nn.Module):\n",
    "    def __init__(self, extra_feature_dim):\n",
    "        super().__init__()\n",
    "        self.backbone = InceptionResnetV1(pretrained='vggface2').eval()\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False  # freeze if desired\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512 + extra_feature_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)  # BMI regression head\n",
    "        )\n",
    "\n",
    "    def forward(self, face_img, extra_features):\n",
    "        face_embed = self.backbone(face_img)\n",
    "        x = torch.cat([face_embed, extra_features], dim=1)\n",
    "        return self.fc(x).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# STEP 6: Prepare Data Loaders\n",
    "# ===============================================\n",
    "train_df = df_final[df_final['is_training'] == 1]\n",
    "test_df = df_final[df_final['is_training'] == 0]\n",
    "\n",
    "train_dataset = BMIDataset(train_df, image_dir=IMAGE_DIR)\n",
    "test_dataset = BMIDataset(test_df, image_dir=IMAGE_DIR)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# STEP 7: Training Loop\n",
    "# ===============================================\n",
    "model = BMIMultitaskModel(extra_feature_dim=len(train_dataset.extra_feature_cols)).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "def train_epoch(model, dataloader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for face, extra, bmi in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        # face, extra, bmi = face.to(DEVICE), extra.to(DEVICE), bmi.to(DEVICE)\n",
    "        face = face.to(DEVICE, non_blocking=True)\n",
    "        extra = extra.to(DEVICE, non_blocking=True)\n",
    "        bmi = bmi.to(DEVICE, non_blocking=True)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(face, extra)\n",
    "        loss = criterion(preds, bmi)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss=1105.1727, Val MAE=32.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss=1053.0999, Val MAE=31.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss=919.8919, Val MAE=28.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss=682.8822, Val MAE=22.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss=403.2838, Val MAE=15.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss=200.6451, Val MAE=9.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss=111.6949, Val MAE=7.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss=83.8536, Val MAE=7.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss=73.8280, Val MAE=6.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|█████     | 48/94 [3:32:56<23:07:01, 1809.17s/it]"
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# STEP 8: Evaluation\n",
    "# ===============================================\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for face, extra, bmi in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            # face, extra, bmi = face.to(DEVICE), extra.to(DEVICE), bmi.to(DEVICE)\n",
    "            face = face.to(DEVICE, non_blocking=True)\n",
    "            extra = extra.to(DEVICE, non_blocking=True)\n",
    "            bmi = bmi.to(DEVICE, non_blocking=True)\n",
    "            \n",
    "            preds = model(face, extra)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_labels.append(bmi.cpu().numpy())\n",
    "    \n",
    "    preds = np.concatenate(all_preds)\n",
    "    labels = np.concatenate(all_labels)\n",
    "    mae = np.mean(np.abs(preds - labels))\n",
    "    return mae\n",
    "\n",
    "# Training loop with early stopping\n",
    "for epoch in range(10):\n",
    "    loss = train_epoch(model, train_loader)\n",
    "    val_mae = evaluate(model, test_loader)\n",
    "    print(f\"Epoch {epoch+1}: Train Loss={loss:.4f}, Val MAE={val_mae:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
